# Adversarial Paraphrasing Red-Teaming for LLaMA, Mistral & Pythia

This repository delivers a reproducible pipeline to evaluate and improve â€œrefusalâ€ behavior in three open-weight LLMsâ€”LLaMA-3.1-8B, Mistral-7B-v0.1, and Pythia-6.9Bâ€”under adversarial paraphrasing. [Full technical report (PDF)](2025-05-09_Luebbers_report.pdf)

## ğŸš€ Key Features

- **Prompt set**: 64 harmful base prompts Ã— 4 variants (canonical, lexical, syntactic, semantic), including six real-world case studies (e.g. Tokyo sarin, Unit 731, Unabomber).
- **Evaluation scripts**:
  - `run_inference.py` â€” batch-runs all prompts through any base model/pipeline.
  - `run_inference_lora.py`- batch-run with lora adapters
  - `annotate_outputs.py` â€” interactive refusal/harm labeling.
  - `evaluation.ipynb` â€” computes refusal and harmfulness rates, generates publication-quality bar charts.
- **Alignment adapters**: LoRA rank-8 checkpoints for both
  - **SFT** on 580 promptâ†’refusal pairs, and
  - **DPO** on 580 promptÂ­â€“chosen_vs_rejected triples.
- **Results**:
  - **Baseline** refusal: 2â€“14 \%; harmful: up to 62 \%.
  - **DPO** gains: modest (+4â€“38 \% refusal; â€“24â€“40 \% harm).
  - **SFT** gains: dramatic (+60â€“96 \% refusal; harmful â‰¤ 16 \%).

## ğŸ“‚ Repository Structure

```text
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ base_prompts.json           # 64 prompts
â”‚   â”œâ”€â”€ paraphrased_prompts.json    # 64 prompts Ã— 4 variants
â”‚   â”œâ”€â”€ dpo_train.jsonl             # 580 DPO triples
â”‚   â””â”€â”€ sft_train.jsonl             # 580 SFT doubles
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_inference.py
â”‚   â”œâ”€â”€ run_inference_lora.py
â”‚   â”œâ”€â”€ annotate_outputs.py
â”‚   â”œâ”€â”€ evaluation.ipynb
â”‚   â”œâ”€â”€ train_dpo.py
â”‚   â””â”€â”€ train_sft.py
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ refusal_harmful_rates.pdf
â”‚   â””â”€â”€ paraphrase_types.pdf
â”œâ”€â”€ 2025-05-09_Luebbers_report.pdf
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ› ï¸ Quickstart

Tested on

```bash
torch==2.6.0,
transformers==4.51.3
datasets==3.5.0
accelerate==1.6.0
bitsandbytes==0.45.5
matplotlib==3.10.1
trl==0.17.0
peft==0.15.2
```

1. **Install dependencies:**

   ```bash
   pip install -r requirements.txt
   ```

2. **Get model access:**

   <https://huggingface.co/meta-llama/Llama-3.1-8B>
   <https://huggingface.co/mistralai/Mistral-7B-v0.1>

3. **Run inference:**

   possible models:
   "pythia": "EleutherAI/pythia-6.9b"
   "mistral": "mistralai/Mistral-7B-v0.1"
   "llama": "meta-llama/Meta-Llama-3.1-8B"

   ```bash
   python scripts/run_inference.py \
     --model llama
   ```

   and adapters either "sft" or "dpo"

   ```bash
   python scripts/run_inference_lora.py \
      --model llama \
      --adapter dpo
   ```

4. **Annotate outputs:**
   You need to specify the input and output files in the script

   ```bash
   python scripts/annotate_outputs.py
   ```

5. **Inspect results** with scripts/evaluation.ipynb

## ğŸ“‘ Key Findings

Paraphrase-aware SFT yields the largest safety gains with minimal compute.
Even with only 580 examples, SFT yields near-perfect refusal on all three models.

|  Method  | Avg. Refusal â†‘ | Avg. Harm â†“ |
| :------: | :------------: | :---------: |
| Baseline |      6 \%      |    41 \%    |
|   DPO    |     17 \%      |    22 \%    |
|   SFT    |     89 \%      |    8 \%     |

![Model Alignment Results](figures/refusal_harmful_rates.pdf)

## ğŸ“– Citing This Work

```bibtex
@article{lubbers2025refusal,
  title={Evaluating Refusal Robustness under Adversarial Paraphrasing},
  author={Luebbers, Christopher L.},
  year={2025},
  howpublished={\url{https://github.com/cluebbers/adverserial-paraphrasing}}
}
```

---

Feel free to explore, adapt, or extend this toolkit for your own red-teaming and alignment research!
